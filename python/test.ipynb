{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d120e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ c·∫Øt th√†nh 4 ƒëo·∫°n.\n",
      "V√≠ d·ª• ƒëo·∫°n 1: Docker cho ph√©p b·∫°n t√°ch ·ª©ng d·ª•ng kh·ªèi c∆° s·ªü h·∫° t·∫ßng ƒë·ªÉ b·∫°n c√≥ th·ªÉ ph√¢n ph·ªëi ph·∫ßn m·ªÅm nhanh ch√≥ng.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. Gi·∫£ l·∫≠p m·ªôt vƒÉn b·∫£n d√†i (V√≠ d·ª•: T√†i li·ªáu h∆∞·ªõng d·∫´n)\n",
    "long_text = \"\"\"\n",
    "Docker l√† m·ªôt n·ªÅn t·∫£ng m·ªü ƒë·ªÉ ph√°t tri·ªÉn, v·∫≠n chuy·ªÉn v√† ch·∫°y c√°c ·ª©ng d·ª•ng. \n",
    "Docker cho ph√©p b·∫°n t√°ch ·ª©ng d·ª•ng kh·ªèi c∆° s·ªü h·∫° t·∫ßng ƒë·ªÉ b·∫°n c√≥ th·ªÉ ph√¢n ph·ªëi ph·∫ßn m·ªÅm nhanh ch√≥ng. \n",
    "V·ªõi Docker, b·∫°n c√≥ th·ªÉ qu·∫£n l√Ω c∆° s·ªü h·∫° t·∫ßng c·ªßa m√¨nh gi·ªëng nh∆∞ c√°ch b·∫°n qu·∫£n l√Ω c√°c ·ª©ng d·ª•ng.\n",
    "Qdrant l√† m·ªôt c∆° s·ªü d·ªØ li·ªáu vector. N√≥ ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a ƒë·ªÉ l∆∞u tr·ªØ v√† truy v·∫•n c√°c vector embedding.\n",
    "\"\"\"\n",
    "\n",
    "# 2. Kh·ªüi t·∫°o b·ªô chia (Splitter)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,      # K√≠ch th∆∞·ªõc m·ªói ƒëo·∫°n (k√Ω t·ª±)\n",
    "    chunk_overlap=20,    # ƒê·ªô ch·ªìng l·∫•n (ƒë·ªÉ gi·ªØ ng·ªØ c·∫£nh gi·ªØa c√°c ƒëo·∫°n c·∫Øt)\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# 3. Th·ª±c hi·ªán c·∫Øt\n",
    "docs = text_splitter.create_documents([long_text])\n",
    "\n",
    "print(f\"ƒê√£ c·∫Øt th√†nh {len(docs)} ƒëo·∫°n.\")\n",
    "print(f\"V√≠ d·ª• ƒëo·∫°n 1: {docs[1].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdd6f3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K√≠ch th∆∞·ªõc vector: 384\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. T·∫£i model (all-MiniLM-L6-v2 l√† model nh·ªè, nhanh, ph·ªï bi·∫øn cho demo)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 2. L·∫•y n·ªôi dung text t·ª´ c√°c ƒëo·∫°n ƒë√£ c·∫Øt ·ªü b∆∞·ªõc 1\n",
    "texts = [d.page_content for d in docs]\n",
    "\n",
    "# 3. Bi·∫øn ƒë·ªïi th√†nh vector\n",
    "embeddings = model.encode(texts)\n",
    "\n",
    "# Ki·ªÉm tra k√≠ch th∆∞·ªõc vector (Th∆∞·ªùng l√† 384 chi·ªÅu v·ªõi model n√†y)\n",
    "vector_size = len(embeddings[1]) \n",
    "print(f\"K√≠ch th∆∞·ªõc vector: {vector_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf7d48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ l∆∞u d·ªØ li·ªáu v√†o Qdrant th√†nh c√¥ng!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_62396\\3989165896.py:10: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
    "import uuid\n",
    "\n",
    "# 1. Kh·ªüi t·∫°o Qdrant (D√πng \":memory:\" ƒë·ªÉ ch·∫°y tr√™n RAM, kh√¥ng c·∫ßn c√†i server)\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "# 2. T·∫°o Collection (gi·ªëng nh∆∞ t·∫°o B·∫£ng trong SQL)\n",
    "collection_name = \"my_knowledge_base\"\n",
    "client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# 3. ƒê∆∞a d·ªØ li·ªáu v√†o Qdrant\n",
    "points = []\n",
    "for idx, (text, vector) in enumerate(zip(texts, embeddings)):\n",
    "    points.append(\n",
    "        PointStruct(\n",
    "            id=idx,  # ID c√≥ th·ªÉ l√† s·ªë ho·∫∑c UUID\n",
    "            vector=vector.tolist(),\n",
    "            payload={\"text\": text}  # L∆∞u l·∫°i text g·ªëc ƒë·ªÉ t√≠ n·ªØa l·∫•y ra ƒë·ªçc\n",
    "        )\n",
    "    )\n",
    "\n",
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")\n",
    "print(\"ƒê√£ l∆∞u d·ªØ li·ªáu v√†o Qdrant th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f8d363f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C√¢u h·ªèi: Docker d√πng ƒë·ªÉ l√†m g√¨?\n",
      "------------------------------\n",
      "ƒê·ªô t∆∞∆°ng ƒë·ªìng: 0.6241\n",
      "N·ªôi dung t√¨m th·∫•y: Docker l√† m·ªôt n·ªÅn t·∫£ng m·ªü ƒë·ªÉ ph√°t tri·ªÉn, v·∫≠n chuy·ªÉn v√† ch·∫°y c√°c ·ª©ng d·ª•ng.\n",
      "------------------------------\n",
      "ƒê·ªô t∆∞∆°ng ƒë·ªìng: 0.5813\n",
      "N·ªôi dung t√¨m th·∫•y: Docker cho ph√©p b·∫°n t√°ch ·ª©ng d·ª•ng kh·ªèi c∆° s·ªü h·∫° t·∫ßng ƒë·ªÉ b·∫°n c√≥ th·ªÉ ph√¢n ph·ªëi ph·∫ßn m·ªÅm nhanh ch√≥ng.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng\n",
    "query = \"Docker d√πng ƒë·ªÉ l√†m g√¨?\"\n",
    "\n",
    "# 2. M√£ h√≥a c√¢u h·ªèi th√†nh vector (d√πng chung model v·ªõi l√∫c n·∫°p d·ªØ li·ªáu)\n",
    "query_vector = model.encode(query).tolist()\n",
    "\n",
    "# 3. T√¨m ki·∫øm trong Qdrant\n",
    "search_result = client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=query_vector,\n",
    "    limit=2  # L·∫•y 2 k·∫øt qu·∫£ t·ªët nh·∫•t\n",
    ")\n",
    "\n",
    "# 4. In k·∫øt qu·∫£\n",
    "print(f\"\\nC√¢u h·ªèi: {query}\")\n",
    "print(\"-\" * 30)\n",
    "for result in search_result:\n",
    "    print(f\"ƒê·ªô t∆∞∆°ng ƒë·ªìng: {result.score:.4f}\")\n",
    "    print(f\"N·ªôi dung t√¨m th·∫•y: {result.payload['text']}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "818725c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√≥n ƒÉn: 'name_vn' ('name_en'). Thu·ªôc lo·∫°i: cat_name. Gi√°: 'price' VND. Ghi ch√∫: 'note'.\n"
     ]
    }
   ],
   "source": [
    "print(f\"M√≥n ƒÉn: 'name_vn' ('name_en'). \"\n",
    "f\"Thu·ªôc lo·∫°i: cat_name. \"\n",
    "f\"Gi√°: 'price' VND. \"\n",
    "f\"Ghi ch√∫: 'note'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39043513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ t·∫°o xong d·ªØ li·ªáu m·∫´u (menu.json, restaurant_info.txt)\n",
      "--------------------------------------------------\n",
      "‚è≥ ƒêang t·∫£i model all-MiniLM-L6-v2...\n",
      "‚úÖ Model ƒë√£ s·∫µn s√†ng!\n",
      "\n",
      "--- B·∫Øt ƒë·∫ßu n·∫°p d·ªØ li·ªáu ---\n",
      "üëÄ [Giai ƒëo·∫°n 1 - ƒê·ªçc File] ƒê√£ ƒë·ªçc menu.json: T√¨m th·∫•y 1 danh m·ª•c.\n",
      "   üëâ [Giai ƒëo·∫°n 2 - Bi·∫øn ƒë·ªïi] JSON -> Text: \"M√≥n ƒÉn: Ph·ªü B√≤ (Beef Noodle Soup). Thu·ªôc lo·∫°i: M√≥n Ch√≠nh. Gi√°: 50000 VND. Ghi ch√∫: ƒê·∫∑c bi·ªát th∆°m ngon.\"\n",
      "   üëâ [Giai ƒëo·∫°n 2 - Bi·∫øn ƒë·ªïi] JSON -> Text: \"M√≥n ƒÉn: B√∫n Ch·∫£ (Grilled Pork Noodles). Thu·ªôc lo·∫°i: M√≥n Ch√≠nh. Gi√°: 60000 VND. Ghi ch√∫: N∆∞·ªõc ch·∫•m gia truy·ªÅn.\"\n",
      "üëÄ [Giai ƒëo·∫°n 1 - ƒê·ªçc File] ƒê√£ ƒë·ªçc info.txt.\n",
      "   üëâ [Giai ƒëo·∫°n 2 - Bi·∫øn ƒë·ªïi] Raw Text -> Chunk: \"Nh√† h√†ng m·ªü c·ª≠a t·ª´ 8:00 s√°ng ƒë·∫øn 10:00 t·ªëi.\"\n",
      "   üëâ [Giai ƒëo·∫°n 2 - Bi·∫øn ƒë·ªïi] Raw Text -> Chunk: \"Ch√∫ng t√¥i mi·ªÖn ph√≠ g·ª≠i xe cho kh√°ch h√†ng.\"\n",
      "\n",
      "‚ö° [Giai ƒëo·∫°n 3 - Vector h√≥a] ƒêang bi·∫øn ƒë·ªïi 4 ƒëo·∫°n vƒÉn th√†nh s·ªë...\n",
      "   üëâ Vector m·∫´u (3 s·ªë ƒë·∫ßu/384): [-0.13363892  0.06800253 -0.03199044] ...\n",
      "\n",
      "üì¶ [Giai ƒëo·∫°n 4 - ƒê√≥ng g√≥i] Chu·∫©n b·ªã ƒë·∫©y 4 Points v√†o Qdrant.\n",
      "‚úÖ [HO√ÄN T·∫§T] ƒê√£ n·∫°p th√†nh c√¥ng v√†o Qdrant Memory!\n",
      "\n",
      "--- üîé TEST T√åM KI·∫æM ---\n",
      "C√¢u h·ªèi: Nh√† h√†ng b√°n m√≥n g√¨ 50000?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_62396\\1969296676.py:107: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  self.client.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K·∫øt qu·∫£ t√¨m th·∫•y: Nh√† h√†ng m·ªü c·ª≠a t·ª´ 8:00 s√°ng ƒë·∫øn 10:00 t·ªëi.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
    "\n",
    "# --- PH·∫¶N 1: GI·∫¢ L·∫¨P M√îI TR∆Ø·ªúNG & D·ªÆ LI·ªÜU ---\n",
    "# 1. T·∫°o Config gi·∫£\n",
    "class Config:\n",
    "    EMBEDDING_MODEL = 'all-MiniLM-L6-v2' # Model nh·ªè ch·∫°y cho nhanh\n",
    "    COLLECTION_NAME = 'demo_collection'\n",
    "    DATA_DIR = '.' # L∆∞u file t·∫°m ngay th∆∞ m·ª•c hi·ªán t·∫°i\n",
    "\n",
    "# 2. T·∫°o file menu.json gi·∫£\n",
    "sample_menu = [\n",
    "    {\n",
    "        \"category\": \"M√≥n Ch√≠nh\",\n",
    "        \"items\": [\n",
    "            {\"id\": \"mc_01\", \"name_vn\": \"Ph·ªü B√≤\", \"name_en\": \"Beef Noodle Soup\", \"price\": 50000, \"note\": \"ƒê·∫∑c bi·ªát th∆°m ngon\"},\n",
    "            {\"id\": \"mc_02\", \"name_vn\": \"B√∫n Ch·∫£\", \"name_en\": \"Grilled Pork Noodles\", \"price\": 60000, \"note\": \"N∆∞·ªõc ch·∫•m gia truy·ªÅn\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "with open('menu.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sample_menu, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 3. T·∫°o file restaurant_info.txt gi·∫£\n",
    "sample_info = \"\"\"\n",
    "Nh√† h√†ng m·ªü c·ª≠a t·ª´ 8:00 s√°ng ƒë·∫øn 10:00 t·ªëi.\n",
    "Ch√∫ng t√¥i mi·ªÖn ph√≠ g·ª≠i xe cho kh√°ch h√†ng.\n",
    "\"\"\"\n",
    "with open('restaurant_info.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_info)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ t·∫°o xong d·ªØ li·ªáu m·∫´u (menu.json, restaurant_info.txt)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- PH·∫¶N 2: LOGIC C·ª¶A B·∫†N (C√ì TH√äM PRINT ƒê·ªÇ SOI D·ªÆ LI·ªÜU) ---\n",
    "\n",
    "class DataIngestor:\n",
    "    def __init__(self, qdrant_client):\n",
    "        self.client = qdrant_client\n",
    "        print(f\"‚è≥ ƒêang t·∫£i model {Config.EMBEDDING_MODEL}...\")\n",
    "        self.encoder = SentenceTransformer(Config.EMBEDDING_MODEL)\n",
    "        print(\"‚úÖ Model ƒë√£ s·∫µn s√†ng!\\n\")\n",
    "        \n",
    "    def load_menu(self, path):\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            print(f\"üëÄ [Giai ƒëo·∫°n 1 - ƒê·ªçc File] ƒê√£ ƒë·ªçc menu.json: T√¨m th·∫•y {len(data)} danh m·ª•c.\")\n",
    "            \n",
    "            docs = []\n",
    "            for category in data:\n",
    "                cat_name = category['category']\n",
    "                for item in category['items']:\n",
    "                    # T·∫°o n·ªôi dung ng·ªØ nghƒ©a\n",
    "                    content = (\n",
    "                        f\"M√≥n ƒÉn: {item['name_vn']} ({item['name_en']}). \"\n",
    "                        f\"Thu·ªôc lo·∫°i: {cat_name}. \"\n",
    "                        f\"Gi√°: {item['price']} VND. \"\n",
    "                        f\"Ghi ch√∫: {item['note']}.\"\n",
    "                    )\n",
    "                    \n",
    "                    # IN RA ƒê·ªÇ KI·ªÇM TRA\n",
    "                    print(f\"   üëâ [Giai ƒëo·∫°n 2 - Bi·∫øn ƒë·ªïi] JSON -> Text: \\\"{content}\\\"\")\n",
    "                    \n",
    "                    docs.append({\"text\": content, \"source\": \"menu\", \"id\": item['id']})\n",
    "            return docs\n",
    "        except FileNotFoundError:\n",
    "            print(\"Warning: Menu file not found.\")\n",
    "            return []\n",
    "\n",
    "    def load_info(self, path):\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Chia nh·ªè\n",
    "            chunks = content.split('\\n')\n",
    "            docs = []\n",
    "            print(f\"üëÄ [Giai ƒëo·∫°n 1 - ƒê·ªçc File] ƒê√£ ƒë·ªçc info.txt.\")\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                if chunk.strip():\n",
    "                    # IN RA ƒê·ªÇ KI·ªÇM TRA\n",
    "                    print(f\"   üëâ [Giai ƒëo·∫°n 2 - Bi·∫øn ƒë·ªïi] Raw Text -> Chunk: \\\"{chunk.strip()}\\\"\")\n",
    "                    docs.append({\"text\": chunk.strip(), \"source\": \"info\", \"id\": f\"info_{i}\"})\n",
    "            return docs\n",
    "        except FileNotFoundError:\n",
    "            print(\"Warning: Info file not found.\")\n",
    "            return []\n",
    "\n",
    "    def ingest(self):\n",
    "        print(\"--- B·∫Øt ƒë·∫ßu n·∫°p d·ªØ li·ªáu ---\")\n",
    "        menu_docs = self.load_menu(os.path.join(Config.DATA_DIR, 'menu.json'))\n",
    "        info_docs = self.load_info(os.path.join(Config.DATA_DIR, 'restaurant_info.txt'))\n",
    "        \n",
    "        all_docs = menu_docs + info_docs\n",
    "        \n",
    "        if not all_docs:\n",
    "            print(\"Kh√¥ng c√≥ d·ªØ li·ªáu.\")\n",
    "            return\n",
    "\n",
    "        # T·∫°o collection (RAM)\n",
    "        self.client.recreate_collection(\n",
    "            collection_name=Config.COLLECTION_NAME,\n",
    "            vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    "        )\n",
    "\n",
    "        print(f\"\\n‚ö° [Giai ƒëo·∫°n 3 - Vector h√≥a] ƒêang bi·∫øn ƒë·ªïi {len(all_docs)} ƒëo·∫°n vƒÉn th√†nh s·ªë...\")\n",
    "        embeddings = self.encoder.encode([d['text'] for d in all_docs])\n",
    "        \n",
    "        # In th·ª≠ vector ƒë·∫ßu ti√™n xem m·∫∑t m≈©i n√≥ ra sao\n",
    "        print(f\"   üëâ Vector m·∫´u (3 s·ªë ƒë·∫ßu/384): {embeddings[0][:3]} ...\")\n",
    "\n",
    "        points = []\n",
    "        for i, doc in enumerate(all_docs):\n",
    "            points.append(PointStruct(\n",
    "                id=i, # L∆∞u √Ω: Code g·ªëc c·ªßa b·∫°n d√πng i l√†m ID, c·∫©n th·∫≠n tr√πng n·∫øu ch·∫°y nhi·ªÅu l·∫ßn\n",
    "                vector=embeddings[i].tolist(),\n",
    "                payload=doc\n",
    "            ))\n",
    "            \n",
    "        print(f\"\\nüì¶ [Giai ƒëo·∫°n 4 - ƒê√≥ng g√≥i] Chu·∫©n b·ªã ƒë·∫©y {len(points)} Points v√†o Qdrant.\")\n",
    "        \n",
    "        self.client.upsert(\n",
    "            collection_name=Config.COLLECTION_NAME,\n",
    "            points=points\n",
    "        )\n",
    "        print(f\"‚úÖ [HO√ÄN T·∫§T] ƒê√£ n·∫°p th√†nh c√¥ng v√†o Qdrant Memory!\")\n",
    "\n",
    "# --- PH·∫¶N 3: CH·∫†Y TH·ª¨ ---\n",
    "if __name__ == \"__main__\":\n",
    "    # D√πng Qdrant ch·∫°y tr√™n RAM (kh√¥ng c·∫ßn c√†i Docker ƒë·ªÉ test)\n",
    "    client = QdrantClient(\":memory:\")\n",
    "    \n",
    "    ingestor = DataIngestor(client)\n",
    "    ingestor.ingest()\n",
    "    \n",
    "    # Test th·ª≠ t√¨m ki·∫øm ƒë·ªÉ ch·ª©ng minh d·ªØ li·ªáu ƒë√£ v√†o\n",
    "    print(\"\\n--- üîé TEST T√åM KI·∫æM ---\")\n",
    "    query = \"Nh√† h√†ng b√°n m√≥n g√¨ 50000?\"\n",
    "    print(f\"C√¢u h·ªèi: {query}\")\n",
    "    \n",
    "    model = SentenceTransformer(Config.EMBEDDING_MODEL)\n",
    "    hits = client.search(\n",
    "        collection_name=Config.COLLECTION_NAME,\n",
    "        query_vector=model.encode(query).tolist(),\n",
    "        limit=1\n",
    "    )\n",
    "    \n",
    "    for hit in hits:\n",
    "        print(f\"K·∫øt qu·∫£ t√¨m th·∫•y: {hit.payload['text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
